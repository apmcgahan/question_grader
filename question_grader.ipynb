{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8548d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Script to evaluate survey responses using LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0fe70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": "# INITIALIZE questions_list\n# Extract questions from the questions.docx file\nquestions_file = \"questions.docx\"\n\nif os.path.exists(questions_file):\n    doc = Document(questions_file)\n    questions_list = []\n    \n    # Extract text from each paragraph in the document\n    for paragraph in doc.paragraphs:\n        text = paragraph.text.strip()\n        # Only add non-empty lines as questions\n        if text:\n            questions_list.append(text)\n    \n    print(f\"Loaded {len(questions_list)} questions from {questions_file}\")\n    print(\"First 3 questions:\")\n    for i, q in enumerate(questions_list[:3], 1):\n        print(f\"  {i}. {q[:100]}{'...' if len(q) > 100 else ''}\")\nelse:\n    print(f\"Warning: {questions_file} not found. Using sample questions.\")\n    questions_list = [\n        \"What is your opinion on the topic?\",\n        \"How would you describe your experience?\",\n        \"What suggestions do you have for improvement?\"\n    ]\n\nprint(f\"\\nTotal questions: {len(questions_list)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98539720",
   "metadata": {},
   "outputs": [],
   "source": "# INITIALIZE spreadsheet_file\nimport openpyxl\nfrom openpyxl import Workbook, load_workbook\nfrom docx import Document\nimport os\n\n# Create or load the output spreadsheet\noutput_file = \"question_grader_output.xlsx\"\n\nif os.path.exists(output_file):\n    spreadsheet_file = load_workbook(output_file)\n    sheet = spreadsheet_file.active\n    print(f\"Loaded existing spreadsheet: {output_file}\")\nelse:\n    spreadsheet_file = Workbook()\n    sheet = spreadsheet_file.active\n    sheet.title = \"Graded Survey\"\n    print(f\"Created new spreadsheet: {output_file}\")\n\nprint(f\"Active sheet: {sheet.title}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb87833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE llm_api_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823b1fd",
   "metadata": {},
   "outputs": [],
   "source": "# Copy survey_data (rows 3 to n) from survey_spreadsheet to questions_spreadsheet\n# Specify the survey file that contains the original responses\nsurvey_file = \"survey_data.xlsx\"  # Change this to your actual survey file\n\nif os.path.exists(survey_file):\n    # Load the survey spreadsheet\n    survey_workbook = load_workbook(survey_file)\n    survey_sheet = survey_workbook.active\n    \n    # Get the dimensions of the survey data\n    max_row = survey_sheet.max_row\n    max_col = survey_sheet.max_column\n    \n    print(f\"Survey file: {survey_file}\")\n    print(f\"Survey sheet: {survey_sheet.title}\")\n    print(f\"Total rows in survey: {max_row}\")\n    print(f\"Copying rows 3 to {max_row}...\")\n    \n    # Copy data from row 3 onwards to the output spreadsheet\n    # Starting at row 3 in the destination as well (rows 1-2 reserved for questions and LLM answers)\n    rows_copied = 0\n    for row_idx in range(3, max_row + 1):\n        for col_idx in range(1, max_col + 1):\n            # Get value from survey sheet\n            cell_value = survey_sheet.cell(row=row_idx, column=col_idx).value\n            # Write to output sheet at the same position\n            sheet.cell(row=row_idx, column=col_idx, value=cell_value)\n        rows_copied += 1\n    \n    print(f\"Successfully copied {rows_copied} rows of survey data\")\n    \n    # Save the spreadsheet\n    spreadsheet_file.save(output_file)\n    print(f\"Saved spreadsheet to {output_file}\")\n    \nelse:\n    print(f\"Warning: Survey file '{survey_file}' not found.\")\n    print(\"Please update the 'survey_file' variable with the correct filename.\")\n    print(\"For now, creating empty spreadsheet with structure ready for data.\")"
  },
  {
   "cell_type": "markdown",
   "id": "c3d7f5d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 1: Populate first row with questions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION populate_questions():\n",
    "#    FOR each question IN questions_list:\n",
    "#        column_index = get_next_available_column()\n",
    "#        write_to_cell(row=1, col=column_index, value=question)\n",
    "#    END FOR\n",
    "#END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc50451",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 2: Generate LLM answers for each question (row 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION generate_llm_answers():\n",
    "#     FOR each question IN row 1:\n",
    "#         column_index = get_column_index(question)\n",
    "        \n",
    "#         # Prompt LLM to answer the question\n",
    "#         prompt = \"Answer the following question: \" + question\n",
    "#         llm_answer = call_llm_api(prompt)\n",
    "        \n",
    "#         # Write LLM answer to row 2\n",
    "#         write_to_cell(row=2, col=column_index, value=llm_answer)\n",
    "#     END FOR\n",
    "# END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1a755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 3: Create evaluation columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f463e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(\"Question grader setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ce408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION create_evaluation_columns():\n",
    "#     question_count = count_columns_with_questions()\n",
    "    \n",
    "#     FOR i = 1 TO question_count:\n",
    "#         # Insert new column after each question column\n",
    "#         question_col = get_column_index_for_question(i)\n",
    "#         evaluation_col = question_col + 1\n",
    "        \n",
    "#         insert_column(at=evaluation_col)\n",
    "        \n",
    "#         # Add header for evaluation column\n",
    "#         evaluation_header = \"Q\" + i + \"_evaluation\"\n",
    "#         write_to_cell(row=1, col=evaluation_col, value=evaluation_header)\n",
    "#     END FOR\n",
    "# END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606a396",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 4: Evaluate each human answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86922999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION evaluate_human_answers():\n",
    "#     question_count = count_questions()\n",
    "    \n",
    "#     FOR question_num = 1 TO question_count:\n",
    "#         # Get question and LLM answer\n",
    "#         question_col = get_column_index_for_question(question_num)\n",
    "#         evaluation_col = question_col + 1\n",
    "        \n",
    "#         question_text = read_cell(row=1, col=question_col)\n",
    "#         known_answer = read_cell(row=2, col=question_col)\n",
    "        \n",
    "#         # Iterate through all human responses (rows 3 to n)\n",
    "#         FOR row = 3 TO last_row_with_data:\n",
    "#             human_answer = read_cell(row=row, col=question_col)\n",
    "            \n",
    "#             # Skip if human answer is empty\n",
    "#             IF human_answer IS empty:\n",
    "#                 CONTINUE\n",
    "#             END IF\n",
    "            \n",
    "#             # Construct evaluation prompt\n",
    "#             evaluation_prompt = \"\"\"\n",
    "#             Question: {question_text}\n",
    "#             Known correct answer: {known_answer}\n",
    "#             Human answer to evaluate: {human_answer}\n",
    "            \n",
    "#             Please evaluate the human answer compared to the known answer.\n",
    "#             Provide a review including:\n",
    "#             - Accuracy assessment\n",
    "#             - Key points covered or missed\n",
    "#             - Overall quality rating\n",
    "#             \"\"\"\n",
    "            \n",
    "#             # Get LLM evaluation\n",
    "#             evaluation_result = call_llm_api(evaluation_prompt)\n",
    "            \n",
    "#             # Write evaluation to spreadsheet\n",
    "#             write_to_cell(row=row, col=evaluation_col, value=evaluation_result)\n",
    "            \n",
    "#         END FOR\n",
    "#     END FOR\n",
    "# END FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50038c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Main execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION main():\n",
    "#     # Step 1: Set up questions\n",
    "#     populate_questions()\n",
    "    \n",
    "#     # Step 2: Get LLM answers\n",
    "#     generate_llm_answers()\n",
    "    \n",
    "#     # Step 3: Create evaluation columns\n",
    "#     create_evaluation_columns()\n",
    "    \n",
    "#     # Step 4: Evaluate all human answers\n",
    "#     evaluate_human_answers()\n",
    "    \n",
    "#     # Save spreadsheet\n",
    "#     save_spreadsheet()\n",
    "    \n",
    "#     PRINT \"Evaluation complete!\"\n",
    "# END FUNCTION\n",
    "\n",
    "# # Run the script\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}