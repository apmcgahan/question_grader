{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8548d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Script to evaluate survey responses using LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0fe70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Configuration\n",
    "INITIALIZE spreadsheet_file\n",
    "INITIALIZE questions_list\n",
    "INITIALIZE llm_api_connection\n",
    "INITIALIZE survey_data (rows 3 to n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98539720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE spreadsheet_file\n",
    "\n",
    "#INITIALIZE questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb87833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE llm_api_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy survey_data (rows 3 to n) from survey_spreadsheet to questions_spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7f5d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 1: Populate first row with questions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION populate_questions():\n",
    "#    FOR each question IN questions_list:\n",
    "#        column_index = get_next_available_column()\n",
    "#        write_to_cell(row=1, col=column_index, value=question)\n",
    "#    END FOR\n",
    "#END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc50451",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 2: Generate LLM answers for each question (row 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION generate_llm_answers():\n",
    "#     FOR each question IN row 1:\n",
    "#         column_index = get_column_index(question)\n",
    "        \n",
    "#         # Prompt LLM to answer the question\n",
    "#         prompt = \"Answer the following question: \" + question\n",
    "#         llm_answer = call_llm_api(prompt)\n",
    "        \n",
    "#         # Write LLM answer to row 2\n",
    "#         write_to_cell(row=2, col=column_index, value=llm_answer)\n",
    "#     END FOR\n",
    "# END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1a755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 3: Create evaluation columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f463e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(\"Question grader setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ce408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION create_evaluation_columns():\n",
    "#     question_count = count_columns_with_questions()\n",
    "    \n",
    "#     FOR i = 1 TO question_count:\n",
    "#         # Insert new column after each question column\n",
    "#         question_col = get_column_index_for_question(i)\n",
    "#         evaluation_col = question_col + 1\n",
    "        \n",
    "#         insert_column(at=evaluation_col)\n",
    "        \n",
    "#         # Add header for evaluation column\n",
    "#         evaluation_header = \"Q\" + i + \"_evaluation\"\n",
    "#         write_to_cell(row=1, col=evaluation_col, value=evaluation_header)\n",
    "#     END FOR\n",
    "# END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606a396",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 4: Evaluate each human answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86922999",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_human_answers(worksheet, anthropic_client):\n    \"\"\"\n    Evaluate each human answer against the LLM's known answer.\n    \n    Args:\n        worksheet: The worksheet object containing questions and answers\n        anthropic_client: Initialized Anthropic client for API calls\n    \"\"\"\n    # Get all values from the worksheet\n    all_values = worksheet.get_all_values()\n    \n    if len(all_values) < 3:\n        print(\"Not enough rows to evaluate (need at least 3 rows)\")\n        return\n    \n    headers = all_values[0]\n    llm_answers = all_values[1]\n    \n    # Find question columns (columns without \"_evaluation\" suffix)\n    question_columns = []\n    for idx, header in enumerate(headers):\n        if header and not header.endswith('_evaluation'):\n            question_columns.append(idx)\n    \n    print(f\"Found {len(question_columns)} questions to evaluate\")\n    \n    # Iterate through all human responses (rows 3 onwards)\n    for row_idx in range(2, len(all_values)):\n        row_number = row_idx + 1  # 1-indexed for spreadsheet\n        human_responses = all_values[row_idx]\n        \n        print(f\"Evaluating row {row_number}...\")\n        \n        # Evaluate each question for this human\n        for question_num, question_col in enumerate(question_columns, start=1):\n            question_text = headers[question_col]\n            known_answer = llm_answers[question_col]\n            \n            # Get human answer\n            if question_col < len(human_responses):\n                human_answer = human_responses[question_col]\n            else:\n                human_answer = \"\"\n            \n            # Skip if human answer is empty\n            if not human_answer or human_answer.strip() == \"\":\n                print(f\"  Q{question_num}: Skipping (no answer)\")\n                continue\n            \n            # Construct evaluation prompt\n            evaluation_prompt = f\"\"\"Question: {question_text}\nKnown correct answer: {known_answer}\nHuman answer to evaluate: {human_answer}\n\nPlease evaluate the human answer compared to the known answer.\nProvide a review including:\n- Accuracy assessment\n- Key points covered or missed\n- Overall quality rating\"\"\"\n            \n            try:\n                # Get LLM evaluation using Anthropic API\n                message = anthropic_client.messages.create(\n                    model=\"claude-3-5-sonnet-20241022\",\n                    max_tokens=1024,\n                    messages=[\n                        {\"role\": \"user\", \"content\": evaluation_prompt}\n                    ]\n                )\n                \n                evaluation_result = message.content[0].text\n                \n                # Write evaluation to the column right after the question column\n                evaluation_col = question_col + 1\n                # Convert to A1 notation (1-indexed)\n                cell_address = f\"{chr(65 + evaluation_col)}{row_number}\"\n                worksheet.update(cell_address, evaluation_result)\n                \n                print(f\"  Q{question_num}: Evaluated and written to {cell_address}\")\n                \n            except Exception as e:\n                error_message = f\"Error evaluating: {str(e)}\"\n                print(f\"  Q{question_num}: {error_message}\")\n                # Write error to evaluation column\n                evaluation_col = question_col + 1\n                cell_address = f\"{chr(65 + evaluation_col)}{row_number}\"\n                worksheet.update(cell_address, error_message)\n    \n    print(\"Human answer evaluation complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "e50038c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Main execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION main():\n",
    "#     # Step 1: Set up questions\n",
    "#     populate_questions()\n",
    "    \n",
    "#     # Step 2: Get LLM answers\n",
    "#     generate_llm_answers()\n",
    "    \n",
    "#     # Step 3: Create evaluation columns\n",
    "#     create_evaluation_columns()\n",
    "    \n",
    "#     # Step 4: Evaluate all human answers\n",
    "#     evaluate_human_answers()\n",
    "    \n",
    "#     # Save spreadsheet\n",
    "#     save_spreadsheet()\n",
    "    \n",
    "#     PRINT \"Evaluation complete!\"\n",
    "# END FUNCTION\n",
    "\n",
    "# # Run the script\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}